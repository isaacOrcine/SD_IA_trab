# Sistema DistribuÃ­do de IA para GeraÃ§Ã£o de Posts do Instagram

Sistema distribuÃ­do com mÃºltiplos agentes de IA para geraÃ§Ã£o inteligente de conteÃºdo para Instagram.

## ğŸ“‹ Arquitetura

```
Trab_SD/
â”œâ”€â”€ agent1-local/       # Agente 1 - Ollama + Llama3.2:1b (Local)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ entrypoint.sh
â”œâ”€â”€ agent2-gemini/      # Agente 2 - Gemini (Cloud) [TODO]
â”œâ”€â”€ api/                # API principal de orquestraÃ§Ã£o [TODO]
â””â”€â”€ docker-compose.yml
```

## ğŸš€ Agent 1 - Modelo Local (Ollama + Llama3.2:1b)

O primeiro agente utiliza **Ollama** rodando localmente com o modelo **llama3.2:1b** (modelo leve e rÃ¡pido).

### Funcionalidades

- âœ… Servidor HTTP com FastAPI na porta 8001
- âœ… Endpoint POST `/generate` para geraÃ§Ã£o de posts
- âœ… Modelo Llama3.2:1b rodando localmente
- âœ… Volume persistente para cache de modelos
- âœ… Health check automÃ¡tico

## ğŸ› ï¸ Como Rodar

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- MÃ­nimo 4GB de RAM disponÃ­vel
- ~1.3GB de espaÃ§o em disco para o modelo

### Iniciar o Sistema

```bash
# Na raiz do projeto
docker-compose up --build
```

**Importante:** O primeiro start levarÃ¡ alguns minutos para:
1. Instalar o Ollama
2. Baixar o modelo llama3.2:1b (~1.3GB)
3. Iniciar o servidor

Aguarde a mensagem: `"Iniciando servidor FastAPI..."`

### Verificar Status

```bash
# Verificar se o agente estÃ¡ online
curl http://localhost:8001/

# Verificar saÃºde do serviÃ§o
curl http://localhost:8001/health
```

## ğŸ“¡ API - Endpoints

### GET `/`
Retorna informaÃ§Ãµes do agente

**Response:**
```json
{
  "agent": "agent1-local",
  "model": "llama3.2:1b",
  "status": "online"
}
```

### GET `/health`
Verifica saÃºde do serviÃ§o Ollama

**Response:**
```json
{
  "status": "healthy",
  "ollama": "online"
}
```

### POST `/generate`
Gera rascunho de post para Instagram

**Request:**
```json
{
  "topic": "ServiÃ§os de computaÃ§Ã£o",
  "style": "casual"
}
```

**Response:**
```json
{
  "draft": "VocÃª tem o potencial de conquistar sua melhor versÃ£o! NÃ£o Ã© sÃ³ sobre o treino, mas tambÃ©m sobre criar um hÃ¡bito que se torne uma parte natural da sua vida. Cada passo vocÃª desafia, cada exercÃ­cio vocÃª supera... Qual Ã© o seu objetivo maior?\n\n#TreinamentoMotivacional #AcademiaDesafia #CriaÃ§Ã£oDeHÃ¡bitos",
  "agent": "agent1-local",
  "model": "llama3.2:1b"
}
```

## ğŸ§ª Exemplos de Teste

### Usando cURL (PowerShell)

```powershell
# Teste bÃ¡sico
curl http://localhost:8001/


# Gerar post profissional sobre tecnologia
curl -X POST http://localhost:8001/generate `
  -H "Content-Type: application/json" `
  -d '{\"topic\": \"inteligÃªncia artificial\", \"style\": \"profissional\"}'

# Gerar post divertido sobre pets
curl -X POST http://localhost:8001/generate `
  -H "Content-Type: application/json" `
  -d '{\"topic\": \"meu cachorro\", \"style\": \"divertido\"}'
```

### Usando Python

```python
import requests

url = "http://localhost:8001/generate"
payload = {
    "topic": "treino na academia",
    "style": "motivacional"
}

response = requests.post(url, json=payload)
result = response.json()

print(f"Post gerado por: {result['agent']}")
print(f"Modelo: {result['model']}")
print(f"\n{result['draft']}")
```

### Usando Postman

1. **Method:** POST
2. **URL:** `http://localhost:8001/generate`
3. **Headers:** `Content-Type: application/json`
4. **Body (raw JSON):**
```json
{
  "topic": "sunset na montanha",
  "style": "inspiracional"
}
```

## ğŸ”§ Gerenciamento

### Ver logs em tempo real
```bash
docker-compose logs -f agent1-local
```

### Parar o sistema
```bash
docker-compose down
```

### Parar e remover volumes (limpar cache de modelos)
```bash
docker-compose down -v
```

### Rebuild completo
```bash
docker-compose down
docker-compose build --no-cache
docker-compose up
```

## ğŸ“Š InformaÃ§Ãµes TÃ©cnicas

### Modelo
- **Nome:** llama3.2:1b
- **Tamanho:** ~1.3GB
- **Fornecedor:** Meta (via Ollama)
- **Contexto:** 128k tokens
- **Ideal para:** GeraÃ§Ã£o rÃ¡pida de texto curto

### Performance Esperada
- **Primeira execuÃ§Ã£o:** 30-60 segundos (download do modelo)
- **ExecuÃ§Ãµes seguintes:** 5-15 segundos por geraÃ§Ã£o
- **RAM necessÃ¡ria:** ~2GB durante geraÃ§Ã£o

### Portas Utilizadas
- **8001:** API FastAPI do Agent 1
- **11434:** Ollama (interno ao container)

### Volumes Persistentes
- `ollama-models`: Cache dos modelos baixados

## ğŸ”œ PrÃ³ximos Passos

- [ ] Implementar Agent 2 com Google Gemini
- [ ] Criar API de orquestraÃ§Ã£o para coordenar agentes
- [ ] Implementar sistema de escolha/ranking de posts
- [ ] Adicionar geraÃ§Ã£o de imagens
- [ ] Interface web para testar os agentes

## ğŸ“ Notas

- O modelo roda completamente **offline** apÃ³s o download inicial
- Resultados podem variar em criatividade e qualidade
- Para melhor qualidade, considere modelos maiores (llama3.2:3b, llama3:8b)
- O primeiro start pode demorar devido ao download do modelo

## âš ï¸ Troubleshooting

### Container nÃ£o inicia
```bash
# Verificar logs
docker-compose logs agent1-local

# Verificar se a porta estÃ¡ disponÃ­vel
netstat -an | findstr "8001"
```

### Modelo nÃ£o baixa
- Verifique conexÃ£o com internet
- Confirme espaÃ§o em disco disponÃ­vel
- Reinicie o container: `docker-compose restart agent1-local`

### Timeout ao gerar
- Aguarde mais tempo na primeira geraÃ§Ã£o (modelo carrega na primeira vez)
- Verifique recursos disponÃ­veis (RAM/CPU)

---

**Desenvolvido para Trabalho de Sistemas DistribuÃ­dos**
