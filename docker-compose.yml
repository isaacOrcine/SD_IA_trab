services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - instagram-ai-network

  agent1-local:
    build:
      context: ./agent1-local
      dockerfile: Dockerfile
    container_name: agent1-local
    ports:
      - "8001:8001"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - instagram-ai-network
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    command: python app.py
    stdin_open: true
    tty: true

  agent2-gemini:
    build:
      context: ./agent2-gemini
      dockerfile: Dockerfile
    container_name: agent2-gemini
    ports:
      - "8002:8002"
    volumes:
      - ./agent2-gemini/outputs:/app/outputs
    networks:
      - instagram-ai-network
    env_file:
      - ./agent2-gemini/.env
    command: python app.py
    stdin_open: true
    tty: true

  orchestrator:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: orchestrator
    environment:
      - AGENT1_URL=http://agent1-local:8001
      - AGENT2_URL=http://agent2-gemini:8002
    networks:
      - instagram-ai-network
    depends_on:
      - agent1-local
      - agent2-gemini
    command: python main.py
    stdin_open: true
    tty: true

networks:
  instagram-ai-network:
    driver: bridge

volumes:
  ollama-models:
    driver: local